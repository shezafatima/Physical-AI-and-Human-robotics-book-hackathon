"use strict";(globalThis.webpackChunkcoursebook_frontend=globalThis.webpackChunkcoursebook_frontend||[]).push([[446],{2300:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>c});var s=i(4848),r=i(8453);const t={sidebar_position:6},l="Chapter 6: Vision-Language-Action (VLA) Systems",o={id:"chapters/chapter6",title:"Chapter 6: Vision-Language-Action (VLA) Systems",description:"Introduction to Vision-Language-Action Systems",source:"@site/docs/chapters/chapter6.md",sourceDirName:"chapters",slug:"/chapters/chapter6",permalink:"/docs/chapters/chapter6",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/chapters/chapter6.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Chapter 5: Humanoid Robot Development",permalink:"/docs/chapters/chapter5"},next:{title:"Chapter 7: Conversational Robotics & GPT Integration",permalink:"/docs/chapters/chapter7"}},a={},c=[{value:"Introduction to Vision-Language-Action Systems",id:"introduction-to-vision-language-action-systems",level:2},{value:"Understanding VLA Architecture",id:"understanding-vla-architecture",level:2},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:3},{value:"End-to-End Learning",id:"end-to-end-learning",level:3},{value:"Key VLA Models and Approaches",id:"key-vla-models-and-approaches",level:2},{value:"RT-1 (Robotics Transformer 1)",id:"rt-1-robotics-transformer-1",level:3},{value:"BC-Z (Behavior Cloning with Z-scoring)",id:"bc-z-behavior-cloning-with-z-scoring",level:3},{value:"FOWM (Following Instructions with Vision and Language Models)",id:"fowm-following-instructions-with-vision-and-language-models",level:3},{value:"Technical Implementation",id:"technical-implementation",level:2},{value:"Data Collection Pipeline",id:"data-collection-pipeline",level:3},{value:"Model Architecture",id:"model-architecture",level:3},{value:"Vision Processing",id:"vision-processing",level:3},{value:"Language Processing",id:"language-processing",level:3},{value:"Action Generation",id:"action-generation",level:3},{value:"Training VLA Systems",id:"training-vla-systems",level:2},{value:"Data Requirements",id:"data-requirements",level:3},{value:"Training Process",id:"training-process",level:3},{value:"Simulation-to-Real Transfer",id:"simulation-to-real-transfer",level:3},{value:"Practical Implementation Example",id:"practical-implementation-example",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:2},{value:"1. Safety and Robustness",id:"1-safety-and-robustness",level:3},{value:"2. Computational Requirements",id:"2-computational-requirements",level:3},{value:"3. Generalization",id:"3-generalization",level:3},{value:"Advanced VLA Techniques",id:"advanced-vla-techniques",level:2},{value:"Hierarchical VLA",id:"hierarchical-vla",level:3},{value:"Interactive Learning",id:"interactive-learning",level:3},{value:"Multi-Robot VLA",id:"multi-robot-vla",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Robustness Metrics",id:"robustness-metrics",level:3},{value:"Future Directions",id:"future-directions",level:2},{value:"Emerging Trends",id:"emerging-trends",level:3},{value:"Research Frontiers",id:"research-frontiers",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"chapter-6-vision-language-action-vla-systems",children:"Chapter 6: Vision-Language-Action (VLA) Systems"}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-vision-language-action-systems",children:"Introduction to Vision-Language-Action Systems"}),"\n",(0,s.jsx)(e.p,{children:"Vision-Language-Action (VLA) systems represent an emerging paradigm in robotics where AI models can perceive visual information, understand natural language commands, and generate appropriate actions. These systems enable more intuitive human-robot interaction by bridging the gap between high-level instructions and low-level robot control."}),"\n",(0,s.jsx)(e.h2,{id:"understanding-vla-architecture",children:"Understanding VLA Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,s.jsx)(e.p,{children:"VLA systems integrate three key modalities:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision"}),": Processing visual information from cameras and sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language"}),": Understanding and generating natural language"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action"}),": Generating robot behaviors and control commands"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"end-to-end-learning",children:"End-to-End Learning"}),"\n",(0,s.jsx)(e.p,{children:"Unlike traditional approaches that separate perception, planning, and control, VLA systems learn to map directly from visual and linguistic inputs to robot actions through deep learning."}),"\n",(0,s.jsx)(e.h2,{id:"key-vla-models-and-approaches",children:"Key VLA Models and Approaches"}),"\n",(0,s.jsx)(e.h3,{id:"rt-1-robotics-transformer-1",children:"RT-1 (Robotics Transformer 1)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Architecture"}),": Transformer-based model for robot learning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Capabilities"}),": Generalizable manipulation skills"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Training"}),": Large-scale robot data with language annotations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Performance"}),": Can execute novel tasks based on language commands"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"bc-z-behavior-cloning-with-z-scoring",children:"BC-Z (Behavior Cloning with Z-scoring)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Approach"}),": Imitation learning with temporal consistency"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Features"}),": Smooth trajectory generation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Advantages"}),": Stable and predictable behavior"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"fowm-following-instructions-with-vision-and-language-models",children:"FOWM (Following Instructions with Vision and Language Models)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Focus"}),": Following complex, multi-step instructions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Capabilities"}),": Long-horizon task execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Integration"}),": Combines planning with low-level control"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"technical-implementation",children:"Technical Implementation"}),"\n",(0,s.jsx)(e.h3,{id:"data-collection-pipeline",children:"Data Collection Pipeline"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Example VLA data collection\nimport numpy as np\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\n\nclass VLADataCollector:\n    def __init__(self):\n        self.image_buffer = []\n        self.joint_buffer = []\n        self.language_buffer = []\n\n    def collect_demonstration(self, instruction, demo_function):\n        """Collect demonstration data for VLA training"""\n        # Record visual observations\n        # Record joint states and actions\n        # Associate with language instruction\n        pass\n\n    def process_trajectory(self, images, joints, instruction):\n        """Process collected trajectory for training"""\n        # Synchronize modalities\n        # Extract relevant features\n        # Format for training\n        pass\n'})}),"\n",(0,s.jsx)(e.h3,{id:"model-architecture",children:"Model Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Vision Encoder \u2500\u2500\u2510\n                 \u251c\u2500\u2500\u2192 Fusion Layer \u2192 Action Decoder\nLanguage Encoder \u2500\u2500\u2518\n\nInputs: [Image, Instruction] \u2192 Outputs: [Robot Actions]\n"})}),"\n",(0,s.jsx)(e.h3,{id:"vision-processing",children:"Vision Processing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CNN Encoders"}),": Feature extraction from images"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision Transformers"}),": Attention-based visual processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-view Fusion"}),": Combining information from multiple cameras"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Detection"}),": Identifying relevant objects in the scene"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"language-processing",children:"Language Processing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transformer Models"}),": BERT, GPT, or specialized language encoders"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tokenization"}),": Converting natural language to model inputs"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Semantic Understanding"}),": Extracting meaning from instructions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Context Awareness"}),": Understanding spatial and temporal context"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"action-generation",children:"Action Generation"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Continuous Control"}),": Generating joint positions/velocities/torques"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Discrete Actions"}),": High-level action selection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Temporal Consistency"}),": Smooth action sequences"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Constraints"}),": Enforcing physical and safety limits"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"training-vla-systems",children:"Training VLA Systems"}),"\n",(0,s.jsx)(e.h3,{id:"data-requirements",children:"Data Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Large-Scale Datasets"}),": Thousands of robot demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Diverse Tasks"}),": Multiple task categories and environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-Modal Annotations"}),": Images, language, and action labels"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Long-Horizon Tasks"}),": Extended sequences for complex behaviors"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"training-process",children:"Training Process"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pre-training"}),": Training on large vision-language datasets"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robot Fine-tuning"}),": Adapting to specific robot platforms"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Iterative Refinement"}),": Improving through interaction"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Validation"}),": Ensuring safe execution"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"simulation-to-real-transfer",children:"Simulation-to-Real Transfer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Domain Randomization"}),": Varying simulation parameters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Synthetic Data"}),": Generating diverse training data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Adversarial Training"}),": Improving robustness"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"System Identification"}),": Calibrating simulation parameters"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"practical-implementation-example",children:"Practical Implementation Example"}),"\n",(0,s.jsx)(e.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport torch\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VLARobotController(Node):\n    def __init__(self):\n        super().__init__(\'vla_controller\')\n\n        # Initialize model\n        self.model = self.load_vla_model()\n\n        # Setup ROS interfaces\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, \'/command\', self.command_callback, 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        self.bridge = CvBridge()\n        self.current_image = None\n        self.pending_command = None\n\n    def image_callback(self, msg):\n        """Process incoming camera images"""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n        self.current_image = cv2.resize(cv_image, (224, 224))\n\n    def command_callback(self, msg):\n        """Process incoming language commands"""\n        self.pending_command = msg.data\n        if self.current_image is not None:\n            self.execute_command()\n\n    def execute_command(self):\n        """Generate and execute robot action"""\n        if self.pending_command and self.current_image is not None:\n            action = self.model.predict(\n                image=self.current_image,\n                instruction=self.pending_command\n            )\n            self.publish_action(action)\n\n    def publish_action(self, action):\n        """Convert model output to robot commands"""\n        cmd_vel = Twist()\n        cmd_vel.linear.x = action[0]  # forward velocity\n        cmd_vel.angular.z = action[1]  # angular velocity\n        self.cmd_vel_pub.publish(cmd_vel)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,s.jsx)(e.h3,{id:"1-safety-and-robustness",children:"1. Safety and Robustness"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Unforeseen Situations"}),": Handling unexpected scenarios"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Physical Safety"}),": Ensuring safe robot behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness"}),": Operating reliably in varied conditions"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-computational-requirements",children:"2. Computational Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Real-time Processing"}),": Meeting timing constraints"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hardware Requirements"}),": Sufficient computational power"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Energy Efficiency"}),": Managing power consumption"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-generalization",children:"3. Generalization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Novel Scenarios"}),": Handling previously unseen situations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Transfer Learning"}),": Adapting to new environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-task Learning"}),": Handling diverse tasks"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"advanced-vla-techniques",children:"Advanced VLA Techniques"}),"\n",(0,s.jsx)(e.h3,{id:"hierarchical-vla",children:"Hierarchical VLA"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"High-Level Planning"}),": Task decomposition and planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Low-Level Control"}),": Fine-grained action execution"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Replanning"}),": Adjusting plans based on execution feedback"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"interactive-learning",children:"Interactive Learning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Human-in-the-Loop"}),": Learning from human corrections"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning from success/failure"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Curriculum Learning"}),": Progressive skill building"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"multi-robot-vla",children:"Multi-Robot VLA"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Coordinated Actions"}),": Multiple robots following instructions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Communication"}),": Robots sharing information"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Allocation"}),": Distributing tasks among robots"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Success Rate"}),": Percentage of successful task completion"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Execution Time"}),": Time to complete tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety Violations"}),": Number of safety-related failures"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Human Preference"}),": User satisfaction ratings"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"robustness-metrics",children:"Robustness Metrics"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Zero-shot Generalization"}),": Performance on unseen tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Adversarial Robustness"}),": Performance under perturbations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Long-horizon Stability"}),": Performance over extended periods"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsx)(e.h3,{id:"emerging-trends",children:"Emerging Trends"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Large Language Models"}),": Integration with advanced LLMs"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Foundation Models"}),": Pre-trained models for robotics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Embodied AI"}),": AI systems with physical interaction capabilities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Continual Learning"}),": Lifelong learning and adaptation"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"research-frontiers",children:"Research Frontiers"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-Modal Reasoning"}),": Complex reasoning across modalities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Social Interaction"}),": Human-aware robot behavior"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Creative Tasks"}),": Robots performing creative activities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Collaborative Robotics"}),": Human-robot teaming"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"In the next chapter, we'll explore conversational robotics and how AI integration enables natural human-robot interaction."})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function l(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:l(n.components),s.createElement(t.Provider,{value:e},n.children)}}}]);